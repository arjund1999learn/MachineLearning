{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6Id1jEnx9ddsXWXOzCHaf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This program creates a convolutional neural network for image recognition\n"
      ],
      "metadata": {
        "id": "2s63IPFaXVZb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MZdBKxeGXFDY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from PIL import Image\n",
        "import io\n",
        "import zipfile\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset\n",
        "\n",
        "Link to dataset: https://drive.google.com/drive/folders/1YPuVjC1qK1inVBkVh5teQRVcT8O9wWBH?usp=sharing\n",
        "\n",
        "You must add a shortcut of Archive.zip to your own drive. This can be done by hitting the \"Dataset\" dropdown -> Organize -> Add Shortcut -> My Drive. After the shortcut is added, the following imports will work correctly."
      ],
      "metadata": {
        "id": "us7Lbck0XOKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Unzip dataset\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Dataset/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp/dataset\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Initialize empty lists\n",
        "X_original = []\n",
        "Y = []\n",
        "\n",
        "map_labels ={'buildings' : 0, 'forest' : 1, 'glacier' : 2, 'mountain' : 3, 'sea' : 4, 'street' : 5}\n",
        "\n",
        "# Define folder path\n",
        "folder_path = '/tmp/dataset/dataset'\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    # Change the current working directory to the \"tmp\" folder\n",
        "    os.chdir(folder_path)\n",
        "else:\n",
        "    print(\"The 'tmp' folder does not exist in your Google Drive.\")\n",
        "\n",
        "# Iterate through main folder to define class labels\n",
        "for label in os.listdir(folder_path):\n",
        "    label_path = os.path.join(folder_path, label)\n",
        "\n",
        "    # Iterate through each subfolder and apply class labels\n",
        "    if os.path.isdir(label_path):\n",
        "        for image_file in os.listdir(label_path):\n",
        "\n",
        "            image_path = os.path.join(label_path, image_file)\n",
        "\n",
        "            img = np.array(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            # Reject any images not 150x150 px\n",
        "            if len(img) != 150: continue\n",
        "\n",
        "            X_original.append(img)\n",
        "\n",
        "            # Append the corresponding label (folder name) to Y_train list\n",
        "            Y.append(map_labels[label])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_original = np.array(X_original)\n",
        "Y = np.array(Y)\n",
        "\n",
        "#Split data into Train and Test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_original, Y, test_size = .2, random_state=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7amNQ8EMXLJ-",
        "outputId": "e1c8377a-2de7-43d3-dc4e-6b7ef0004095"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ql280olwbT8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model and Test Model"
      ],
      "metadata": {
        "id": "YC3E5rT0bJ-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape = (150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((3, 3)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(8))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=10,\n",
        "                    validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RxM_4U_XYOq",
        "outputId": "03784f60-349f-4b87-c70b-b51305e53497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 68/424 [===>..........................] - ETA: 8:57 - loss: 9.8799 - accuracy: 0.2316"
          ]
        }
      ]
    }
  ]
}