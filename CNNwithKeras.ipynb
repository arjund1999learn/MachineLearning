{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6Id1jEnx9ddsXWXOzCHaf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This program creates a convolutional neural network for image recognition\n"
      ],
      "metadata": {
        "id": "2s63IPFaXVZb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MZdBKxeGXFDY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from PIL import Image\n",
        "import io\n",
        "import zipfile\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset\n",
        "\n",
        "Link to dataset: https://drive.google.com/drive/folders/1YPuVjC1qK1inVBkVh5teQRVcT8O9wWBH?usp=sharing\n",
        "\n",
        "You must add a shortcut of Archive.zip to your own drive. This can be done by hitting the \"Dataset\" dropdown -> Organize -> Add Shortcut -> My Drive. After the shortcut is added, the following imports will work correctly."
      ],
      "metadata": {
        "id": "us7Lbck0XOKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Unzip dataset\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Dataset/dataset.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp/dataset\")\n",
        "zip_ref.close()\n",
        "\n",
        "# Initialize empty lists\n",
        "X_original = []\n",
        "Y = []\n",
        "\n",
        "map_labels ={'buildings' : 0, 'forest' : 1, 'glacier' : 2, 'mountain' : 3, 'sea' : 4, 'street' : 5}\n",
        "\n",
        "# Define folder path\n",
        "folder_path = '/tmp/dataset/dataset'\n",
        "\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    # Change the current working directory to the \"tmp\" folder\n",
        "    os.chdir(folder_path)\n",
        "else:\n",
        "    print(\"The 'tmp' folder does not exist in your Google Drive.\")\n",
        "\n",
        "# Iterate through main folder to define class labels\n",
        "for label in os.listdir(folder_path):\n",
        "    label_path = os.path.join(folder_path, label)\n",
        "\n",
        "    # Iterate through each subfolder and apply class labels\n",
        "    if os.path.isdir(label_path):\n",
        "        for image_file in os.listdir(label_path):\n",
        "\n",
        "            image_path = os.path.join(label_path, image_file)\n",
        "\n",
        "            img = np.array(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            # Reject any images not 150x150 px\n",
        "            if len(img) != 150: continue\n",
        "\n",
        "            X_original.append(img)\n",
        "\n",
        "            # Append the corresponding label (folder name) to Y_train list\n",
        "            Y.append(map_labels[label])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_original = np.array(X_original)\n",
        "Y = np.array(Y)\n",
        "\n",
        "#Split data into Train and Test data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_original, Y, test_size = .2, random_state=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7amNQ8EMXLJ-",
        "outputId": "e1c8377a-2de7-43d3-dc4e-6b7ef0004095"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ql280olwbT8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model and Test Model"
      ],
      "metadata": {
        "id": "YC3E5rT0bJ-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape = (150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((3, 3)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(8))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train, epochs=10,\n",
        "                    validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RxM_4U_XYOq",
        "outputId": "3177098e-e710-4ff9-f27e-58033bde3659"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "424/424 [==============================] - 682s 2s/step - loss: 2.7419 - accuracy: 0.4211 - val_loss: 1.2039 - val_accuracy: 0.5589\n",
            "Epoch 2/10\n",
            "424/424 [==============================] - 724s 2s/step - loss: 0.9935 - accuracy: 0.6117 - val_loss: 0.9689 - val_accuracy: 0.6242\n",
            "Epoch 3/10\n",
            "424/424 [==============================] - 725s 2s/step - loss: 0.8386 - accuracy: 0.6838 - val_loss: 0.9652 - val_accuracy: 0.6290\n",
            "Epoch 4/10\n",
            "424/424 [==============================] - 683s 2s/step - loss: 0.6791 - accuracy: 0.7499 - val_loss: 0.8027 - val_accuracy: 0.7072\n",
            "Epoch 5/10\n",
            "424/424 [==============================] - 714s 2s/step - loss: 0.5283 - accuracy: 0.8126 - val_loss: 0.8895 - val_accuracy: 0.6860\n",
            "Epoch 6/10\n",
            "424/424 [==============================] - 679s 2s/step - loss: 0.4255 - accuracy: 0.8493 - val_loss: 1.0486 - val_accuracy: 0.6925\n",
            "Epoch 7/10\n",
            "424/424 [==============================] - 676s 2s/step - loss: 0.3115 - accuracy: 0.8881 - val_loss: 0.9508 - val_accuracy: 0.7137\n",
            "Epoch 8/10\n",
            "424/424 [==============================] - 713s 2s/step - loss: 0.2804 - accuracy: 0.9002 - val_loss: 1.1649 - val_accuracy: 0.7099\n",
            "Epoch 9/10\n",
            "424/424 [==============================] - 720s 2s/step - loss: 0.2306 - accuracy: 0.9198 - val_loss: 1.6392 - val_accuracy: 0.6535\n",
            "Epoch 10/10\n",
            "424/424 [==============================] - 701s 2s/step - loss: 0.2185 - accuracy: 0.9242 - val_loss: 1.3961 - val_accuracy: 0.7202\n"
          ]
        }
      ]
    }
  ]
}